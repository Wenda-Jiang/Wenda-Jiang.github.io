<!DOCTYPE html><html>

<head>
<meta charset="utf-8">
<title></title>
<style>
@font-face {
  font-family: octicons-anchor;
  src: url(data:font/woff;charset=utf-8;base64,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) format('woff');
}

body {
  background-color: white;
}

.markdown-body {
  min-width: 200px;
  max-width: 760px;
  margin: 0 auto;
  padding: 30px;

  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
  color: #333;
  overflow: hidden;
  font-family: "Helvetica Neue", Helvetica, "Segoe UI", Arial, freesans, sans-serif;
  font-size: 16px;
  line-height: 1.6;
  word-wrap: break-word;
}

.markdown-body a {
  background: transparent;
}

.markdown-body a:active,
.markdown-body a:hover {
  outline: 0;
}

.markdown-body strong {
  font-weight: bold;
}

.markdown-body h1 {
  font-size: 2em;
  margin: 0.67em 0;
}

.markdown-body img {
  border: 0;
}

.markdown-body hr {
  -moz-box-sizing: content-box;
  box-sizing: content-box;
  height: 0;
}

.markdown-body pre {
  overflow: auto;
}

.markdown-body code,
.markdown-body kbd,
.markdown-body pre {
  font-family: monospace, monospace;
  font-size: 1em;
}

.markdown-body input {
  color: inherit;
  font: inherit;
  margin: 0;
}

.markdown-body html input[disabled] {
  cursor: default;
}

.markdown-body input {
  line-height: normal;
}

.markdown-body input[type="checkbox"] {
  -moz-box-sizing: border-box;
  box-sizing: border-box;
  padding: 0;
}

.markdown-body table {
  border-collapse: collapse;
  border-spacing: 0;
}

.markdown-body td,
.markdown-body th {
  padding: 0;
}

.markdown-body * {
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}

.markdown-body input {
  font: 13px/1.4 Helvetica, arial, freesans, clean, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol";
}

.markdown-body a {
  color: #4183c4;
  text-decoration: none;
}

.markdown-body a:hover,
.markdown-body a:focus,
.markdown-body a:active {
  text-decoration: underline;
}

.markdown-body hr {
  height: 0;
  margin: 15px 0;
  overflow: hidden;
  background: transparent;
  border: 0;
  border-bottom: 1px solid #ddd;
}

.markdown-body hr:before {
  display: table;
  content: "";
}

.markdown-body hr:after {
  display: table;
  clear: both;
  content: "";
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-top: 15px;
  margin-bottom: 15px;
  line-height: 1.1;
}

.markdown-body h1 {
  font-size: 30px;
}

.markdown-body h2 {
  font-size: 21px;
}

.markdown-body h3 {
  font-size: 16px;
}

.markdown-body h4 {
  font-size: 14px;
}

.markdown-body h5 {
  font-size: 12px;
}

.markdown-body h6 {
  font-size: 11px;
}

.markdown-body blockquote {
  margin: 0;
}

.markdown-body ul,
.markdown-body ol {
  padding: 0;
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body ol ol,
.markdown-body ul ol {
  list-style-type: lower-roman;
}

.markdown-body ul ul ol,
.markdown-body ul ol ol,
.markdown-body ol ul ol,
.markdown-body ol ol ol {
  list-style-type: lower-alpha;
}

.markdown-body dd {
  margin-left: 0;
}

.markdown-body code {
  font: 12px Consolas, "Liberation Mono", Menlo, Courier, monospace;
}

.markdown-body pre {
  margin-top: 0;
  margin-bottom: 0;
  font: 12px Consolas, "Liberation Mono", Menlo, Courier, monospace;
}

.markdown-body kbd {
  background-color: #e7e7e7;
  background-image: -webkit-linear-gradient(#fefefe, #e7e7e7);
  background-image: linear-gradient(#fefefe, #e7e7e7);
  background-repeat: repeat-x;
  border-radius: 2px;
  border: 1px solid #cfcfcf;
  color: #000;
  padding: 3px 5px;
  line-height: 10px;
  font: 11px Consolas, "Liberation Mono", Menlo, Courier, monospace;
  display: inline-block;
}

.markdown-body>*:first-child {
  margin-top: 0 !important;
}

.markdown-body>*:last-child {
  margin-bottom: 0 !important;
}

.markdown-body .anchor {
  position: absolute;
  top: 0;
  bottom: 0;
  left: 0;
  display: block;
  padding-right: 6px;
  padding-left: 30px;
  margin-left: -30px;
}

.markdown-body .anchor:focus {
  outline: none;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  position: relative;
  margin-top: 1em;
  margin-bottom: 16px;
  font-weight: bold;
  line-height: 1.4;
}

.markdown-body h1 .octicon-link,
.markdown-body h2 .octicon-link,
.markdown-body h3 .octicon-link,
.markdown-body h4 .octicon-link,
.markdown-body h5 .octicon-link,
.markdown-body h6 .octicon-link {
  display: none;
  color: #000;
  vertical-align: middle;
}

.markdown-body h1:hover .anchor,
.markdown-body h2:hover .anchor,
.markdown-body h3:hover .anchor,
.markdown-body h4:hover .anchor,
.markdown-body h5:hover .anchor,
.markdown-body h6:hover .anchor {
  height: 1em;
  padding-left: 8px;
  margin-left: -30px;
  line-height: 1;
  text-decoration: none;
}

.markdown-body h1:hover .anchor .octicon-link,
.markdown-body h2:hover .anchor .octicon-link,
.markdown-body h3:hover .anchor .octicon-link,
.markdown-body h4:hover .anchor .octicon-link,
.markdown-body h5:hover .anchor .octicon-link,
.markdown-body h6:hover .anchor .octicon-link {
  display: inline-block;
}

.markdown-body h1 {
  padding-bottom: 0.3em;
  font-size: 2.25em;
  line-height: 1.2;
  border-bottom: 1px solid #eee;
}

.markdown-body h2 {
  padding-bottom: 0.3em;
  font-size: 1.75em;
  line-height: 1.225;
  border-bottom: 1px solid #eee;
}

.markdown-body h3 {
  font-size: 1.5em;
  line-height: 1.43;
}

.markdown-body h4 {
  font-size: 1.25em;
}

.markdown-body h5 {
  font-size: 1em;
}

.markdown-body h6 {
  font-size: 1em;
  color: #777;
}

.markdown-body p,
.markdown-body blockquote,
.markdown-body ul,
.markdown-body ol,
.markdown-body dl,
.markdown-body table,
.markdown-body pre {
  margin-top: 0;
  margin-bottom: 16px;
}

.markdown-body hr {
  height: 4px;
  padding: 0;
  margin: 16px 0;
  background-color: #e7e7e7;
  border: 0 none;
}

.markdown-body ul,
.markdown-body ol {
  padding-left: 2em;
}

.markdown-body ul ul,
.markdown-body ul ol,
.markdown-body ol ol,
.markdown-body ol ul {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body li>p {
  margin-top: 16px;
}

.markdown-body dl {
  padding: 0;
}

.markdown-body dl dt {
  padding: 0;
  margin-top: 16px;
  font-size: 1em;
  font-style: italic;
  font-weight: bold;
}

.markdown-body dl dd {
  padding: 0 16px;
  margin-bottom: 16px;
}

.markdown-body blockquote {
  padding: 0 15px;
  color: #777;
  border-left: 4px solid #ddd;
}

.markdown-body blockquote>:first-child {
  margin-top: 0;
}

.markdown-body blockquote>:last-child {
  margin-bottom: 0;
}

.markdown-body table {
  display: block;
  width: 100%;
  overflow: auto;
  word-break: normal;
  word-break: keep-all;
}

.markdown-body table th {
  font-weight: bold;
}

.markdown-body table th,
.markdown-body table td {
  padding: 6px 13px;
  border: 1px solid #ddd;
}

.markdown-body table tr {
  background-color: #fff;
  border-top: 1px solid #ccc;
}

.markdown-body table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

.markdown-body img {
  max-width: 100%;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}

.markdown-body code {
  padding: 0;
  padding-top: 0.2em;
  padding-bottom: 0.2em;
  margin: 0;
  font-size: 85%;
  background-color: rgba(0,0,0,0.04);
  border-radius: 3px;
}

.markdown-body code:before,
.markdown-body code:after {
  letter-spacing: -0.2em;
  content: "\00a0";
}

.markdown-body pre>code {
  padding: 0;
  margin: 0;
  font-size: 100%;
  word-break: normal;
  white-space: pre;
  background: transparent;
  border: 0;
}

.markdown-body .highlight {
  margin-bottom: 16px;
}

.markdown-body .highlight pre,
.markdown-body pre {
  padding: 16px;
  overflow: auto;
  font-size: 85%;
  line-height: 1.45;
  background-color: #f7f7f7;
  border-radius: 3px;
}

.markdown-body .highlight pre {
  margin-bottom: 0;
  word-break: normal;
}

.markdown-body pre {
  word-wrap: normal;
}

.markdown-body pre code {
  display: inline;
  max-width: initial;
  padding: 0;
  margin: 0;
  overflow: initial;
  line-height: inherit;
  word-wrap: normal;
  background-color: transparent;
  border: 0;
}

.markdown-body pre code:before,
.markdown-body pre code:after {
  content: normal;
}

.markdown-body .highlight {
  background: #fff;
}

.markdown-body .highlight .mf,
.markdown-body .highlight .mh,
.markdown-body .highlight .mi,
.markdown-body .highlight .mo,
.markdown-body .highlight .il,
.markdown-body .highlight .m {
  color: #945277;
}

.markdown-body .highlight .s,
.markdown-body .highlight .sb,
.markdown-body .highlight .sc,
.markdown-body .highlight .sd,
.markdown-body .highlight .s2,
.markdown-body .highlight .se,
.markdown-body .highlight .sh,
.markdown-body .highlight .si,
.markdown-body .highlight .sx,
.markdown-body .highlight .s1 {
  color: #df5000;
}

.markdown-body .highlight .kc,
.markdown-body .highlight .kd,
.markdown-body .highlight .kn,
.markdown-body .highlight .kp,
.markdown-body .highlight .kr,
.markdown-body .highlight .kt,
.markdown-body .highlight .k,
.markdown-body .highlight .o {
  font-weight: bold;
}

.markdown-body .highlight .kt {
  color: #458;
}

.markdown-body .highlight .c,
.markdown-body .highlight .cm,
.markdown-body .highlight .c1 {
  color: #998;
  font-style: italic;
}

.markdown-body .highlight .cp,
.markdown-body .highlight .cs {
  color: #999;
  font-weight: bold;
}

.markdown-body .highlight .cs {
  font-style: italic;
}

.markdown-body .highlight .n {
  color: #333;
}

.markdown-body .highlight .na,
.markdown-body .highlight .nv,
.markdown-body .highlight .vc,
.markdown-body .highlight .vg,
.markdown-body .highlight .vi {
  color: #008080;
}

.markdown-body .highlight .nb {
  color: #0086B3;
}

.markdown-body .highlight .nc {
  color: #458;
  font-weight: bold;
}

.markdown-body .highlight .no {
  color: #094e99;
}

.markdown-body .highlight .ni {
  color: #800080;
}

.markdown-body .highlight .ne {
  color: #990000;
  font-weight: bold;
}

.markdown-body .highlight .nf {
  color: #945277;
  font-weight: bold;
}

.markdown-body .highlight .nn {
  color: #555;
}

.markdown-body .highlight .nt {
  color: #000080;
}

.markdown-body .highlight .err {
  color: #a61717;
  background-color: #e3d2d2;
}

.markdown-body .highlight .gd {
  color: #000;
  background-color: #fdd;
}

.markdown-body .highlight .gd .x {
  color: #000;
  background-color: #faa;
}

.markdown-body .highlight .ge {
  font-style: italic;
}

.markdown-body .highlight .gr {
  color: #aa0000;
}

.markdown-body .highlight .gh {
  color: #999;
}

.markdown-body .highlight .gi {
  color: #000;
  background-color: #dfd;
}

.markdown-body .highlight .gi .x {
  color: #000;
  background-color: #afa;
}

.markdown-body .highlight .go {
  color: #888;
}

.markdown-body .highlight .gp {
  color: #555;
}

.markdown-body .highlight .gs {
  font-weight: bold;
}

.markdown-body .highlight .gu {
  color: #800080;
  font-weight: bold;
}

.markdown-body .highlight .gt {
  color: #aa0000;
}

.markdown-body .highlight .ow {
  font-weight: bold;
}

.markdown-body .highlight .w {
  color: #bbb;
}

.markdown-body .highlight .sr {
  color: #017936;
}

.markdown-body .highlight .ss {
  color: #8b467f;
}

.markdown-body .highlight .bp {
  color: #999;
}

.markdown-body .highlight .gc {
  color: #999;
  background-color: #EAF2F5;
}

.markdown-body .octicon {
  font: normal normal 16px octicons-anchor;
  line-height: 1;
  display: inline-block;
  text-decoration: none;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.markdown-body .octicon-link:before {
  content: '\f05c';
}

.markdown-body .task-list-item {
  list-style-type: none;
}

.markdown-body .task-list-item+.task-list-item {
  margin-top: 3px;
}

.markdown-body .task-list-item input {
  float: left;
  margin: 0.3em 0 0.25em -1.6em;
  vertical-align: middle;
}

/*

github.com style (c) Vasily Polovnyov <vast@whiteants.net>

*/

.hljs {
  display: block;
  overflow-x: auto;
  padding: 0.5em;
  color: #333;
  background: #f8f8f8;
  -webkit-text-size-adjust: none;
}

.hljs-comment,
.diff .hljs-header {
  color: #998;
  font-style: italic;
}

.hljs-keyword,
.css .rule .hljs-keyword,
.hljs-winutils,
.nginx .hljs-title,
.hljs-subst,
.hljs-request,
.hljs-status {
  color: #333;
  font-weight: bold;
}

.hljs-number,
.hljs-hexcolor,
.ruby .hljs-constant {
  color: #008080;
}

.hljs-string,
.hljs-tag .hljs-value,
.hljs-doctag,
.tex .hljs-formula {
  color: #d14;
}

.hljs-title,
.hljs-id,
.scss .hljs-preprocessor {
  color: #900;
  font-weight: bold;
}

.hljs-list .hljs-keyword,
.hljs-subst {
  font-weight: normal;
}

.hljs-class .hljs-title,
.hljs-type,
.vhdl .hljs-literal,
.tex .hljs-command {
  color: #458;
  font-weight: bold;
}

.hljs-tag,
.hljs-tag .hljs-title,
.hljs-rule .hljs-property,
.django .hljs-tag .hljs-keyword {
  color: #000080;
  font-weight: normal;
}

.hljs-attribute,
.hljs-variable,
.lisp .hljs-body,
.hljs-name {
  color: #008080;
}

.hljs-regexp {
  color: #009926;
}

.hljs-symbol,
.ruby .hljs-symbol .hljs-string,
.lisp .hljs-keyword,
.clojure .hljs-keyword,
.scheme .hljs-keyword,
.tex .hljs-special,
.hljs-prompt {
  color: #990073;
}

.hljs-built_in {
  color: #0086b3;
}

.hljs-preprocessor,
.hljs-pragma,
.hljs-pi,
.hljs-doctype,
.hljs-shebang,
.hljs-cdata {
  color: #999;
  font-weight: bold;
}

.hljs-deletion {
  background: #fdd;
}

.hljs-addition {
  background: #dfd;
}

.diff .hljs-change {
  background: #0086b3;
}

.hljs-chunk {
  color: #aaa;
}


</style>
<script type="text/javascript"
     src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
 </script>
<style> @media print{ .hljs{overflow: visible; word-wrap: break-word !important;} }</style></head><body><div class="markdown-body">
<h1 id="toc_0">QA(二)：利用Attention机制，带着问题阅读原文</h1>

<p><a href="https://arxiv.org/pdf/1608.07905.pdf">MACHINE COMPREHENSION USING MATCH-LSTM AND ANSWER POINTER</a></p>

<h2 id="toc_1">摘要</h2>

<p>本文介绍一种结合 math-LSTM 和Pointer Net利用end-end的来解决QA问题的方式</p>

<h2 id="toc_2">模型</h2>

<p>最主要的还是 match-LSTM：有两个句子，一个是前提，另外一个是假设，match-LSTM序列化的经过假设的每一个词，然后预测前提是否继承自假设。</p>

<p>简单的说：带着问题去阅读原文，然后用得到的信息去回答问题</p>

<ol>
<li>先利用LSTM阅读一遍passage，得到输出的encoding 序列</li>
<li>然后带着question的信息，重新将passage的每个词输入LSTM，再次得到passage的encoding信息。但是这次的输入不仅仅只有passage的信息，还包含这个词和question的关联信息，它和qustion的关联信息的计算方式就是我们在seq2seq模型里面最常用的attention机制。</li>
<li>然后将信息输入answer模块，生成答案</li>
</ol>

<p>下面介绍详细的模型</p>

<h3 id="toc_3">1. 预处理层LSTM Preprocessing层</h3>

<p>首先对文本和问题分别单独用LSTM进行单向的encoder<br/>
\[<br/>
\begin{matrix}<br/>
H^p=LSTM(P)  \newline<br/>
H^q=LSTM(Q)  \newline<br/>
\end{matrix}<br/>
\]<br/>
\(H^p \in R^{[l, p]}, H^q \in R^{[l, q]}\)<br/>
l 是LSTMcell的隐藏层大小，p和q分别是文本passage 和 问题question的长度<br/>
代码很简单，两个序列分别经过LSTM序列模型，就得到encoder向量。</p>

<pre><code class="language-python">lstm_cell_question = tf.nn.rnn_cell_impl.BasicLSTMCell(l, state_is_tuple=True)
encoded_question, q_rep = tf.nn.dynamic_rnn(lstm_cell_question, question,masks_question,dtype=tf.float32)

lstm_cell_passage = tf.nn.rnn_cell_impl.BasicLSTMCell(l, state_is_tuple=True)
encoded_passage, p_rep = tf.nn.dynamic_rnn(lstm_cell_passage, passage,masks_passage, dtype=tf.float32)
</code></pre>

<h3 id="toc_4">2. Match-LSTM 层</h3>

<p>带着qustion来阅读passage，利用的是利用了 <a href="https://arxiv.org/pdf/1409.0473.pdf">Bahdanau Attention机制</a>机制，具体可以见该论文。</p>

<p>但是为了详细描述，在这里还是详细的描述一遍：</p>

<p>整体的思路可以看作我们在decoder passage，我们聚焦的是qustion向量：<br/>
\(h^r_i = LSTM(z_i,h^r_{i-1})\)</p>

<p>由attention机制我们可以知道，这里的\(z_i\)是融合passage的input和对qustion的attention信息：<br/>
\[ z_i =  \left[<br/>
\begin{matrix}<br/>
 h_i^p      \newline<br/>
 f(H^q)     \newline<br/>
\end{matrix}<br/>
\right]<br/>
\]</p>

<p>\(H^p\)是prcess层利用LSTM将passage预处理后得到的，<br/>
第i个词的向量为\(h^p_i \in R^l\)，我们在\(h^p_i\)之后加一个qustion相关的信息，</p>

<p>令：\(f(H^q)=H^q\alpha_i\)</p>

<p>其中\(\alpha_i\)是文本passage里面的第i个词，首先计算第i个词和question里面每一个词的相关性权重</p>

<p>\(\alpha_i\)就是attention的alignment model：</p>

<p>\[<br/>
\begin{matrix}<br/>
G_i=tanh(W^qH^q+(W^p h^p_i + W^r h^r_{i-1} + b^p) \bigotimes e_Q) \newline<br/>
\alpha_i = softmax(w^tG_i + b\bigotimes e_Q) <br/>
\end{matrix}<br/>
\]</p>

<pre><code class="language-python"># tensorflow 里面有现成的BahdanauAttention类
match_lstm = BahdanauAttention(l, q)
</code></pre>

<p>这样我们就得到了\(\alpha_i\)</p>

<p>这样我们可以完整的迭代这个序列模型：<br/>
\[h^r_i = LSTM(z_i,h^r_{i-1})\]</p>

<p>同理我们将passage倒叙，可以得到倒叙的LSTM模型</p>

<p>\[ \hat{h_i^r} = LSTM(\hat{z_i}, \hat{h^r_{i-1}}) \]</p>

<p>我们令forward和backward得到的转台矩阵分别为\(H^r,\hat{H^r}\), 我们把两个矩阵直接连接起来得到最终的状态矩阵<br/>
\[ H_r= \left[<br/>
\begin{matrix}<br/>
 H^r      \newline<br/>
 \hat{H^r}     \newline<br/>
\end{matrix}<br/>
\right]<br/>
\]<br/>
\(H_r \in R^{[2l, p]}\)</p>

<pre><code class="language-python"># LSTM Cell
cell = BasicLSTMCell(l, state_is_tuple=True)
lstm_attender = AttentionWrapper(cell, match_lstm)
reverse_encoded_passage = _reverse(encoded_passage)

# bi-dir LSTM
output_attender_fw, _ = tf.nn.dynamic_rnn(lstm_attender, encoded_passage, dtype=tf.float32, scope=&quot;rnn&quot;)
output_attender_bw, _ = tf.nn.dynamic_rnn(lstm_attender, reverse_encoded_passage, dtype=tf.float32, scope=&quot;rnn&quot;)

output_attender_bw = _reverse(output_attender_bw)

# concat
output_attender = tf.concat([output_attender_fw, output_attender_bw], axis=-1)
</code></pre>

<h3 id="toc_5">3. Answer Pointer</h3>

<p>Answer Pointer的思想是从Pointer Net得到的，<br/>
它将\(H^r\)作为输入，生成答案有两种方式：<br/>
1. sequence，自动生成答案序列, 序列里面的词是从passage里面选取出来的<br/>
2. boundary，答案从passage里面截取，模型生成的是开始和结束下标</p>

<h4 id="toc_6">Sequence</h4>

<p>假设我们的答案序列为：<br/>
\(a=(a_1,a_2,&hellip;)\)<br/>
其中\(a_i\)为选择出来答案的词在原文passage里面的下标位置，<br/>
\(a_i \in [1, P + 1]\), 其中第P + 1 是一个特殊的字符，表示答案的终止，当预测出来的词是终止字符时，结束答案生成。</p>

<p>简单的方式是像机器翻译一样，直接利用LSTM做decoder处理：<br/>
假设\(a_1,a_2,..,a_{k-1}\)<br/>
\[<br/>
\begin{matrix}<br/>
O_k = LSTM(a_{k_1}, h_{k-1}) \newline<br/>
p(a_k|O_k)=argmax_{P + 1} (softmax(WO_k))<br/>
\end{matrix}<br/>
\]<br/>
找到passage里面概率最大的词的就可以了</p>

<p>这里也利用上节讲的Bahdanau Attention机制，<br/>
在预测第k个答案的词时，我们先计算出一个权重向量<br/>
\(\beta_k\)用来表示在[1, P+1]位置的词，各个词的权重</p>

<p>先得到隐藏向量：<br/>
\(h_k^a=LSTM([H^r;0], h_{k-1}^a )\)</p>

<p>计算权重：<br/>
\[<br/>
\begin{matrix}<br/>
F_k=tanh(V[H^r;0] + (W^ah_{k-1}^a + b^a)\bigotimes e_{P + 1} ) \newline<br/>
\beta_k = softmax(v^tF_k + c \bigotimes e_{P + 1}) <br/>
\end{matrix}<br/>
\]</p>

<p>然后\(p(a_k=j)=\beta_{k,j}\)</p>

<p>\(\beta_{k,j}\)最大的一个下标就是\(a_k\)的值</p>

<p>代码和match-LSTM基本一致</p>

<h4 id="toc_7">Boundary</h4>

<p>这种模型很简单，答案\(a=(a_s,a_e)\)只有两个值</p>

<p>\(P(a|H_r)=P(a_s|H_r)p(a_e|a_s,H_r )\)<br/>
实际过程中，可以直接用<br/>
\(P(a|H_r)=P(a_s|H_r)P(a_e|H_r)\)，选取概率最大的\(a_s, a_e\)pair<br/>
计算方式和上面sequene模型一样，只是有两点不同</p>

<ol>
<li>没有显示的结束标记，所有在LSTM input的时候不要补0 </li>
<li>\(a_s \le a_e\)</li>
</ol>

<h2 id="toc_8">实验</h2>

<p>使用预训练过的词向量<br/>
Boundary模式的answer层忧郁是固定的，可以用bi-dir LSTM<br/>
Boundary模式要注意避免截取的段落过长</p>

<h2 id="toc_9">后续改进版本</h2>

<p><a href="https://pdfs.semanticscholar.org/f9f0/8511f77c29ff948e146434dfb23608d3deb5.pdf">Question Answering Using Match-LSTM and Answer Pointer</a></p>

<h3 id="toc_10">1. 预处理层LSTM Preprocessing层</h3>

<p>单向LSTM -&gt; BiLSTM</p>

<p>\[<br/>
H_q=<br/>
\left[<br/>
\begin{matrix}<br/>
h_q^{for} \newline<br/>
h_q^{back}<br/>
\end{matrix}<br/>
\right]<br/>
\]</p>

<p>\[<br/>
H_p=<br/>
\left[<br/>
\begin{matrix}<br/>
h_p^{for} \newline<br/>
h_p^{back}<br/>
\end{matrix}<br/>
\right]<br/>
\]</p>

<h3 id="toc_11">2. regularization</h3>

<p>在所有LSTM外面都加上Dropout，droupout概率为0.2</p>

<h3 id="toc_12">3. 调整op，size大小</h3>

<p>不同的optimal Adam -&gt; AdaMax<br/>
不同的batch-szie <br/>
gradient clipping =0.5</p>

<h3 id="toc_13">实验效果</h3>

<p><img src="media/15123803633991/15126399426010.jpg" alt=""/></p>

<p>图中可以看出，虽然改进的Final Match-LSTM 在训练集上面指标下降，但是在测试集上面是有提升的<br/>
其实在训练过程中dropout和batch_normal真的是很好的两种方式</p>

<p><a href="https://pdfs.semanticscholar.org/3f0f/ce70ab5122ce742daabb8f81473fcb633c85.pdf">Extending Match-LSTM</a></p>

<p>这个工作并没有对Match-Lstm层做任何改动</p>

<h3 id="toc_14">1. 预处理层LSTM Preprocessing层增加attention</h3>

<p>P，Q互为attention聚焦<br/>
\[<br/>
\begin{matrix}<br/>
H^p=LSTM(P, attention(Q))  \newline<br/>
H^q=LSTM(Q, attention(P))  \newline<br/>
\end{matrix}<br/>
\]</p>

<p>这个改进其实更近一步，在预处理阶段就带着对方的信息处理每一个词</p>

<h3 id="toc_15">2. Conditional Span Decoder</h3>

<p>在上文的处理中，answer start 和 end是独立处理的，这里还是恢复以来关系：</p>

<p>\(P(a_e=i,a_s=j|H_r)=P(a_e=i|H_r)=P(a_s=j|H_r)P(a_e=i|a_s=j,H_r)\)</p>

<p>其实如果做到以来，直接用LSTM其实就是上述的概率<br/>
这里作者做了些改进：<br/>
\(H_r\)经过softmax后得到\(a_s\)的概率向量\(h_{s,j}\)<br/>
\(p(a_s=j|H_r)=h_{s,j}=softMax[H_r])\) </p>

<p>然后\(p_{s,j}\)和\(H_r\)一起作为LSTM的输入</p>

<p>\(h_{e,j}=LSTM([p_{s,j},H_r] ))\)</p>

<p>\(p(a_e=i,a_s=j|H_r)=softmax(h_{e,j})\)</p>

<p><img src="media/15123803633991/15126452871063.jpg" alt=""/></p>

<p>graph 对比图</p>

<br><br><br><br>

</div></body>

</html>
